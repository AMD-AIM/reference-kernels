# name: 3_moe_mxfp4

files:
  - {"name": "submission.py", "source": "@SUBMISSION@"}
  - {"name": "task.py", "source": "task.py"}
  - {"name": "utils.py", "source": "../utils.py"}
  - {"name": "reference.py", "source": "reference.py"}
  - {"name": "eval.py", "source": "../eval.py"}

lang: "py"

description: |
  Implement a DeepSeek-R1 style MXFP4 Mixture of Experts (MoE) fused kernel on MI355X.

  The fused_moe kernel performs the following for each token:
    1. Route token to top-k experts based on pre-computed routing weights
    2. For each active expert, compute: gate_proj = SiLU(x @ W_gate), up_proj = x @ W_up
    3. Compute intermediate = gate_proj * up_proj (SwiGLU-style)
    4. Compute output = intermediate @ W_down
    5. Weighted sum of expert outputs

  All expert weights (W_gate, W_up, W_down) are in MXFP4 format with e8m0 block scales (block_size=32).
  The W_gate and W_up are fused into a single w13 weight. Weights are pre-shuffled for CK kernel layout.

  DeepSeek-R1 MoE specs:
    - hidden_size = 7168, moe_intermediate_size = 2048
    - 256 routed experts, top-8 routing
    - 58 MoE layers (layer 3-60)

  Three deployment scenarios (TP=8):
    - bs=4:   decode, all 256 experts local, intermediate TP-split to 256
    - bs=64:  EP=8, 32 experts per GPU, full intermediate=2048
    - bs=256: EP=8, 32 experts per GPU, full intermediate=2048

  Input:
    - hidden_states:    [M, 7168]                    bf16
    - w13_weight:       [E, 2*d_expert_pad, 3584]    fp4x2 (shuffled)
    - w2_weight:        [E, 7168, d_expert_pad//2]   fp4x2 (shuffled)
    - w13_weight_scale: shuffled e8m0 block scales
    - w2_weight_scale:  shuffled e8m0 block scales
    - topk_weights:     [M, 8]                       float32
    - topk_ids:         [M, 8]                       int32
    - config:           dict with dimensions

  Output:
    - output: [M, 7168] bf16

config:
  main: "eval.py"

templates:
  Python: "submission.py"

test_timeout: 540
benchmark_timeout: 540
ranked_timeout: 840
ranking_by: "geom"

tests:
  - {"dhidden": 4096, "dexpert": 1024, "nroutedexperts": 16, "nexpertspertoken": 4, "bs": 8, "seed": 9371}
  - {"dhidden": 7168, "dexpert": 2048, "nroutedexperts": 32, "nexpertspertoken": 8, "bs": 32, "seed": 2291}
  - {"dhidden": 4096, "dexpert": 1536, "nroutedexperts": 64, "nexpertspertoken": 6, "bs": 128, "seed": 81934}

benchmarks:
  # EP off, bs 4-1024
  - {"dhidden": 7168, "dexpert": 256, "nroutedexperts": 256, "nexpertspertoken": 8, "bs": 4, "seed": 9371}
  - {"dhidden": 7168, "dexpert": 256, "nroutedexperts": 256, "nexpertspertoken": 8, "bs": 64, "seed": 2291}
  - {"dhidden": 7168, "dexpert": 256, "nroutedexperts": 256, "nexpertspertoken": 8, "bs": 256, "seed": 81934}
  - {"dhidden": 7168, "dexpert": 256, "nroutedexperts": 256, "nexpertspertoken": 8, "bs": 1024, "seed": 81934}
  # EP on, bs 64-1024
  - {"dhidden": 7168, "dexpert": 2048, "nroutedexperts": 32, "nexpertspertoken": 8, "bs": 64, "seed": 2291}
  - {"dhidden": 7168, "dexpert": 2048, "nroutedexperts": 32, "nexpertspertoken": 8, "bs": 256, "seed": 81934}
  - {"dhidden": 7168, "dexpert": 2048, "nroutedexperts": 32, "nexpertspertoken": 8, "bs": 1024, "seed": 81934}
