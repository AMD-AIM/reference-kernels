# name: 3_moe_mxfp4

files:
  - {"name": "submission.py", "source": "@SUBMISSION@"}
  - {"name": "task.py", "source": "task.py"}
  - {"name": "utils.py", "source": "../utils.py"}
  - {"name": "reference.py", "source": "reference.py"}
  - {"name": "eval.py", "source": "../eval.py"}

lang: "py"

description: |
  Implement a DeepSeek-R1 style MXFP4 Mixture of Experts (MoE) fused kernel on MI355X.

  The fused_moe kernel performs the following:

    Given M tokens, each routed to top_k experts with pre-computed weights:

    For each token i and its assigned expert j (with routing weight w_ij):
      1. Stage 1 (gate_up GEMM + activation):
         - gate = x_i @ W_gate_j.T                      # [d_hidden] x [d_expert, d_hidden].T -> [d_expert]
         - up   = x_i @ W_up_j.T                        # [d_hidden] x [d_expert, d_hidden].T -> [d_expert]
         - intermediate = SiLU(gate) * up                # element-wise SwiGLU activation, -> [d_expert]
         (W_gate and W_up are fused as gate_up_weight so this is a single GEMM + fused activation)

      2. Stage 2 (down GEMM):
         - expert_out = intermediate @ W_down_j.T        # [d_expert] x [d_hidden, d_expert].T -> [d_hidden]

      3. Weighted reduction:
         - output_i += w_ij * expert_out                 # accumulate across top_k experts

    All GEMMs use MXFP4 quantized weights (fp4x2 data + e8m0 block scales, block_size=32).
    The AITER CK kernel fuses all of the above into a 2-stage pipeline across all tokens and experts.

  All expert weights (W_gate, W_up, W_down) are in MXFP4 format with e8m0 block scales (block_size=32).
  The W_gate and W_up are fused into a single gate_up weight. Both raw and pre-shuffled (for CK kernel layout)
  versions of all weights and scales are provided.

  DeepSeek-R1 MoE specs:
    - hidden_size = 7168, moe_intermediate_size = 2048
    - 256 routed experts, top-8 routing
    - 58 MoE layers (layer 3-60)

  Input:
    - hidden_states:                  [M, d_hidden]                          bf16
    - gate_up_weight:                 [E, 2*d_expert_pad, d_hidden_pad//2]   fp4x2 (raw, before shuffle)
    - down_weight:                    [E, d_hidden_pad, d_expert_pad//2]     fp4x2 (raw, before shuffle)
    - gate_up_weight_scale:           [E, 2*d_expert_pad, d_hidden_pad//32]  e8m0  (raw, before shuffle)
    - down_weight_scale:              [E, d_hidden_pad, d_expert_pad//32]    e8m0  (raw, before shuffle)
    - gate_up_weight_shuffled:        [E, 2*d_expert_pad, d_hidden_pad//2]   fp4x2 (pre-shuffled)
    - down_weight_shuffled:           [E, d_hidden_pad, d_expert_pad//2]     fp4x2 (pre-shuffled)
    - gate_up_weight_scale_shuffled:  [padded, flat]                         e8m0  (pre-shuffled)
    - down_weight_scale_shuffled:     [padded, flat]                         e8m0  (pre-shuffled)
    - topk_weights:                   [M, top_k]                             float32
    - topk_ids:                       [M, top_k]                             int32
    - config:                         dict with dimensions

  Output:
    - output: [M, 7168] bf16

config:
  main: "eval.py"

templates:
  Python: "submission.py"

test_timeout: 540
benchmark_timeout: 540
ranked_timeout: 840
ranking_by: "geom"

tests:
  - {"dhidden": 4096, "dexpert": 1024, "nroutedexperts": 16, "nexpertspertoken": 4, "bs": 8, "seed": 9371}
  - {"dhidden": 7168, "dexpert": 2048, "nroutedexperts": 32, "nexpertspertoken": 8, "bs": 32, "seed": 2291}
  - {"dhidden": 4096, "dexpert": 1536, "nroutedexperts": 64, "nexpertspertoken": 6, "bs": 128, "seed": 81934}

benchmarks:
  # EP off, bs 4-1024
  - {"dhidden": 7168, "dexpert": 256, "nroutedexperts": 256, "nexpertspertoken": 8, "bs": 4, "seed": 9371}
  - {"dhidden": 7168, "dexpert": 256, "nroutedexperts": 256, "nexpertspertoken": 8, "bs": 64, "seed": 2291}
  - {"dhidden": 7168, "dexpert": 256, "nroutedexperts": 256, "nexpertspertoken": 8, "bs": 256, "seed": 81934}
  - {"dhidden": 7168, "dexpert": 256, "nroutedexperts": 256, "nexpertspertoken": 8, "bs": 1024, "seed": 81934}
  # EP on, bs 64-1024
  - {"dhidden": 7168, "dexpert": 2048, "nroutedexperts": 32, "nexpertspertoken": 8, "bs": 64, "seed": 2291}
  - {"dhidden": 7168, "dexpert": 2048, "nroutedexperts": 32, "nexpertspertoken": 8, "bs": 256, "seed": 81934}
  - {"dhidden": 7168, "dexpert": 2048, "nroutedexperts": 32, "nexpertspertoken": 8, "bs": 1024, "seed": 81934}
