# name: 3_moe_mxfp4

files:
  - {"name": "submission.py", "source": "@SUBMISSION@"}
  - {"name": "task.py", "source": "task.py"}
  - {"name": "utils.py", "source": "../utils.py"}
  - {"name": "reference.py", "source": "reference.py"}
  - {"name": "eval.py", "source": "../eval.py"}

lang: "py"

description: |
  Implement a DeepSeek-R1 style MXFP4 Mixture of Experts (MoE) fused kernel on MI355X.

  The fused_moe kernel performs the following for each token:
    1. Route token to top-k experts based on pre-computed routing weights
    2. For each active expert, compute: gate_proj = SiLU(x @ W_gate), up_proj = x @ W_up
    3. Compute intermediate = gate_proj * up_proj (SwiGLU-style)
    4. Compute output = intermediate @ W_down
    5. Weighted sum of expert outputs

  All expert weights (W_gate, W_up, W_down) are in MXFP4 format with e8m0 block scales (block_size=32).
  The W_gate and W_up are fused into a single gate_up weight. Both raw and pre-shuffled (for CK kernel layout)
  versions of all weights and scales are provided.

  DeepSeek-R1 MoE specs:
    - hidden_size = 7168, moe_intermediate_size = 2048
    - 256 routed experts, top-8 routing
    - 58 MoE layers (layer 3-60)

  Input:
    - hidden_states:                  [M, d_hidden]                          bf16
    - gate_up_weight:                 [E, 2*d_expert_pad, d_hidden_pad//2]   fp4x2 (raw)
    - down_weight:                    [E, d_hidden_pad, d_expert_pad//2]     fp4x2 (raw)
    - gate_up_weight_scale:           [E, 2*d_expert_pad, d_hidden_pad//32]  e8m0  (raw)
    - down_weight_scale:              [E, d_hidden_pad, d_expert_pad//32]    e8m0  (raw)
    - gate_up_weight_shuffled:        [E, 2*d_expert_pad, d_hidden_pad//2]   fp4x2 (shuffled for CK)
    - down_weight_shuffled:           [E, d_hidden_pad, d_expert_pad//2]     fp4x2 (shuffled for CK)
    - gate_up_weight_scale_shuffled:  [padded, flat]                         e8m0  (shuffled for CK)
    - down_weight_scale_shuffled:     [padded, flat]                         e8m0  (shuffled for CK)
    - topk_weights:                   [M, top_k]                             float32
    - topk_ids:                       [M, top_k]                             int32
    - config:                         dict with dimensions

  Output:
    - output: [M, 7168] bf16

config:
  main: "eval.py"

templates:
  Python: "submission.py"

test_timeout: 540
benchmark_timeout: 540
ranked_timeout: 840
ranking_by: "geom"

tests:
  - {"dhidden": 4096, "dexpert": 1024, "nroutedexperts": 16, "nexpertspertoken": 4, "bs": 8, "seed": 9371}
  - {"dhidden": 7168, "dexpert": 2048, "nroutedexperts": 32, "nexpertspertoken": 8, "bs": 32, "seed": 2291}
  - {"dhidden": 4096, "dexpert": 1536, "nroutedexperts": 64, "nexpertspertoken": 6, "bs": 128, "seed": 81934}

benchmarks:
  # EP off, bs 4-1024
  - {"dhidden": 7168, "dexpert": 256, "nroutedexperts": 256, "nexpertspertoken": 8, "bs": 4, "seed": 9371}
  - {"dhidden": 7168, "dexpert": 256, "nroutedexperts": 256, "nexpertspertoken": 8, "bs": 64, "seed": 2291}
  - {"dhidden": 7168, "dexpert": 256, "nroutedexperts": 256, "nexpertspertoken": 8, "bs": 256, "seed": 81934}
  - {"dhidden": 7168, "dexpert": 256, "nroutedexperts": 256, "nexpertspertoken": 8, "bs": 1024, "seed": 81934}
  # EP on, bs 64-1024
  - {"dhidden": 7168, "dexpert": 2048, "nroutedexperts": 32, "nexpertspertoken": 8, "bs": 64, "seed": 2291}
  - {"dhidden": 7168, "dexpert": 2048, "nroutedexperts": 32, "nexpertspertoken": 8, "bs": 256, "seed": 81934}
  - {"dhidden": 7168, "dexpert": 2048, "nroutedexperts": 32, "nexpertspertoken": 8, "bs": 1024, "seed": 81934}
